{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "import time\n",
    "\n",
    "from model import *\n",
    "from rays_util import *\n",
    "from render import *\n",
    "from load_llff import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(fn, chunk):\n",
    "    \"\"\"\n",
    "    Constructs a version of 'fn' that applies to smaller batches.\n",
    "    \"\"\"\n",
    "    if chunk is None:\n",
    "        return fn\n",
    "    def ret(inputs, viewdirs):\n",
    "        return torch.cat([fn(inputs[i:i+chunk], viewdirs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_rays(rays_flat, chunk=1024*32, **kwargs):\n",
    "    \"\"\"\n",
    "    Render rays in smaller minibatches to avoid OOM.\n",
    "    \"\"\"\n",
    "    all_ret = {}\n",
    "    for i in range(0, rays_flat.shape[0], chunk):\n",
    "        ret = render_rays(rays_flat[i:i+chunk], **kwargs)\n",
    "        for k in ret:\n",
    "            if k not in all_ret:\n",
    "                all_ret[k] = []\n",
    "            all_ret[k].append(ret[k])\n",
    "\n",
    "    all_ret = {k : torch.cat(all_ret[k], 0) for k in all_ret}\n",
    "    return all_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_network(inputs, viewdirs, fn, embed_fn, embeddirs_fn, netchunk=1024*64):\n",
    "    \"\"\"\n",
    "    Prepares inputs and applies network 'fn'.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs_flat = torch.reshape(inputs, [-1, inputs.shape[-1]])\n",
    "    embedded = embed_fn(inputs_flat)\n",
    "    \n",
    "    if viewdirs is not None:\n",
    "        input_dirs = viewdirs[:,None].expand(inputs.shape)\n",
    "        input_dirs_flat = torch.reshape(input_dirs, [-1, input_dirs.shape[-1]])\n",
    "        embedded_dirs = embeddirs_fn(input_dirs_flat)\n",
    "        #embedded = torch.cat([embedded, embedded_dirs], -1)\n",
    "\n",
    "    outputs_flat = batchify(fn, netchunk)(embedded, viewdirs=embedded_dirs)\n",
    "    outputs = torch.reshape(outputs_flat, list(inputs.shape[:-1]) + [outputs_flat.shape[-1]])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(H, W, K, chunk=1024*32, rays=None, c2w=None, use_ndc=True,\n",
    "                  near=0., far=1.,\n",
    "                  use_viewdirs=False, c2w_staticcam=None,\n",
    "                  **kwargs):\n",
    "    \"\"\"Render rays\n",
    "    Args:\n",
    "      H: int. Height of image in pixels.\n",
    "      W: int. Width of image in pixels.\n",
    "      focal: float. Focal length of pinhole camera.\n",
    "      chunk: int. Maximum number of rays to process simultaneously. Used to\n",
    "        control maximum memory usage. Does not affect final results.\n",
    "      rays: array of shape [2, batch_size, 3]. Ray origin and direction for\n",
    "        each example in batch.\n",
    "      c2w: array of shape [3, 4]. Camera-to-world transformation matrix.\n",
    "      ndc: bool. If True, represent ray origin, direction in NDC coordinates.\n",
    "      near: float or array of shape [batch_size]. Nearest distance for a ray.\n",
    "      far: float or array of shape [batch_size]. Farthest distance for a ray.\n",
    "      use_viewdirs: bool. If True, use viewing direction of a point in space in model.\n",
    "      c2w_staticcam: array of shape [3, 4]. If not None, use this transformation matrix for \n",
    "       camera while using other c2w argument for viewing directions.\n",
    "    Returns:\n",
    "      rgb_map: [batch_size, 3]. Predicted RGB values for rays.\n",
    "      disp_map: [batch_size]. Disparity map. Inverse of depth.\n",
    "      acc_map: [batch_size]. Accumulated opacity (alpha) along a ray.\n",
    "      extras: dict with everything returned by render_rays().\n",
    "    \"\"\"\n",
    "    f = K[0][0]\n",
    "\n",
    "    if c2w is not None:\n",
    "        # special case to render full image\n",
    "        rays_o, rays_d = get_rays(H, W, f, c2w)\n",
    "    else:\n",
    "        # use provided ray batch\n",
    "        rays_o, rays_d = rays\n",
    "\n",
    "    if use_viewdirs:\n",
    "        # provide ray directions as input\n",
    "        viewdirs = rays_d\n",
    "        if c2w_staticcam is not None:\n",
    "            # special case to visualize effect of viewdirs\n",
    "            rays_o, rays_d = get_rays(H, W, f, c2w_staticcam)\n",
    "        viewdirs = viewdirs / torch.norm(viewdirs, dim=-1, keepdim=True)\n",
    "        viewdirs = torch.reshape(viewdirs, [-1,3]).float()\n",
    "\n",
    "    sh = rays_d.shape # [..., 3]\n",
    "    if use_ndc:\n",
    "        # for forward facing scenes\n",
    "        rays_o, rays_d = ndc_rays(H, W, f, 1., rays_o, rays_d)\n",
    "\n",
    "    # Create ray batch\n",
    "    rays_o = torch.reshape(rays_o, [-1,3]).float()\n",
    "    rays_d = torch.reshape(rays_d, [-1,3]).float()\n",
    "\n",
    "    near, far = near * torch.ones_like(rays_d[...,:1]), far * torch.ones_like(rays_d[...,:1])\n",
    "    rays = torch.cat([rays_o, rays_d, near, far], -1)\n",
    "    if use_viewdirs:\n",
    "        rays = torch.cat([rays, viewdirs], -1)\n",
    "\n",
    "    # Render and reshape\n",
    "    all_ret = batchify_rays(rays, chunk, **kwargs)\n",
    "    for k in all_ret:\n",
    "        k_sh = list(sh[:-1]) + list(all_ret[k].shape[1:])\n",
    "        all_ret[k] = torch.reshape(all_ret[k], k_sh)\n",
    "\n",
    "    k_extract = ['rgb_map', 'disp_map', 'acc_map']\n",
    "    ret_list = [all_ret[k] for k in k_extract]\n",
    "    ret_dict = {k : all_ret[k] for k in all_ret if k not in k_extract}\n",
    "    return ret_list + [ret_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_path(render_poses, hwf, K, chunk, render_kwargs, gt_imgs=None, savedir=None, render_factor=0):\n",
    "\n",
    "    H, W, focal = hwf\n",
    "\n",
    "    if render_factor!=0:\n",
    "        # Render downsampled for speed\n",
    "        H = H//render_factor\n",
    "        W = W//render_factor\n",
    "        focal = focal/render_factor\n",
    "\n",
    "    rgbs = []\n",
    "    disps = []\n",
    "\n",
    "    t = time.time()\n",
    "    for i, c2w in enumerate(tqdm(render_poses)):\n",
    "        print(i, time.time() - t)\n",
    "        t = time.time()\n",
    "        rgb, disp, acc, _ = render(H, W, K, chunk=chunk, c2w=c2w[:3,:4], **render_kwargs)\n",
    "        rgbs.append(rgb.cpu().numpy())\n",
    "        disps.append(disp.cpu().numpy())\n",
    "        if i==0:\n",
    "            print(rgb.shape, disp.shape)\n",
    "\n",
    "        \"\"\"\n",
    "        if gt_imgs is not None and render_factor==0:\n",
    "            p = -10. * np.log10(np.mean(np.square(rgb.cpu().numpy() - gt_imgs[i])))\n",
    "            print(p)\n",
    "        \"\"\"\n",
    "\n",
    "        if savedir is not None:\n",
    "            rgb8 = to8b(rgbs[-1])\n",
    "            filename = os.path.join(savedir, '{:03d}.png'.format(i))\n",
    "            imageio.imwrite(filename, rgb8)\n",
    "\n",
    "\n",
    "    rgbs = np.stack(rgbs, 0)\n",
    "    disps = np.stack(disps, 0)\n",
    "\n",
    "    return rgbs, disps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "# Model\n",
    "d_filter = 256          # Dimensions of linear layer filters\n",
    "n_layers = 8            # Number of layers in network bottleneck\n",
    "skip = [4]               # Layers at which to apply input residual\n",
    "use_fine_model = True   # If set, creates a fine model\n",
    "d_filter_fine = 256     # Dimensions of linear layer filters of fine network\n",
    "n_layers_fine = 8       # Number of layers in fine network bottleneck\n",
    "\n",
    "# Stratified sampling\n",
    "n_samples = 64         # Number of spatial samples per ray\n",
    "perturb = False         # If set, applies noise to sample positions\n",
    "inverse_depth = False  # If set, samples points linearly in inverse depth\n",
    "\n",
    "# Hierarchical sampling\n",
    "N_importance = 64   # Number of samples per ray\n",
    "perturb_hierarchical = False  # If set, applies noise to sample positions\n",
    "\n",
    "# Encoder\n",
    "d_input = 3           # Number of input dimensions\n",
    "n_freqs = 10          # Number of encoding functions for samples\n",
    "log_sampling = True      # If set, frequencies scale in log space\n",
    "use_viewdirs = True   # If set, use view direction as input\n",
    "n_freqs_views = 4     # Number of encoding functions for views\n",
    "\n",
    "# Training\n",
    "n_iters = 10000+1\n",
    "batch_size = 1024         # Number of rays per gradient step (power of 2)\n",
    "one_image_per_step = False   # One image per gradient step (disables batching)\n",
    "chunksize = 1024           # Modify as needed to fit in GPU memory\n",
    "center_crop = False          # Crop the center of image (one_image_per_)\n",
    "center_crop_iters = 50      # Stop cropping center after this many epochs\n",
    "display_rate = 25          # Display test output every X epochs\n",
    "\n",
    "start = 0\n",
    "basedir = \".\\\\logs\"\n",
    "datadir = \".\\\\fern\"\n",
    "expname = \"fern_test\"\n",
    "save_rate = 25\n",
    "\n",
    "llff_hold = 8\n",
    "\n",
    "use_ndc = True\n",
    "render_test = False\n",
    "\n",
    "# Function kwargs\n",
    "stratified_sampling_kwargs = {\n",
    "    'n_samples': n_samples,\n",
    "    'perturb': perturb,\n",
    "    'inverse_depth': inverse_depth\n",
    "}\n",
    "hierarchical_sampling_kwargs = {\n",
    "    'perturb': perturb\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_models():\n",
    "  \"\"\"\n",
    "  Initialize models, encoders, and optimizer for training.\n",
    "  \"\"\"\n",
    "\n",
    "  # Encoders\n",
    "  encoder = PositionalEncoder(d_input, n_freqs, log_sampling=log_sampling)\n",
    "  encode = lambda x: encoder(x)\n",
    "\n",
    "  # View direction encoders\n",
    "  if use_viewdirs:\n",
    "    encoder_viewdirs = PositionalEncoder(d_input, n_freqs_views,\n",
    "                                        log_sampling=log_sampling)\n",
    "    encode_viewdirs = lambda x: encoder_viewdirs(x)\n",
    "    d_viewdirs = encoder_viewdirs.d_output\n",
    "    test_dir = d_viewdirs\n",
    "  else:\n",
    "    encode_viewdirs = None\n",
    "    d_viewdirs = None\n",
    "\n",
    "  # Models\n",
    "  model = NeRF(encoder.d_output, n_layers=n_layers, d_filter=d_filter, skip=skip,\n",
    "              d_viewdirs=d_viewdirs)\n",
    "  model.to(device)\n",
    "  model_params = list(model.parameters())\n",
    "  if use_fine_model:\n",
    "    fine_model = NeRF(encoder.d_output, n_layers=n_layers, d_filter=d_filter, skip=skip,\n",
    "                      d_viewdirs=d_viewdirs)\n",
    "    fine_model.to(device)\n",
    "    model_params = model_params + list(fine_model.parameters())\n",
    "  else:\n",
    "    fine_model = None\n",
    "  \n",
    "  network_query_fn = lambda inputs, viewdirs, network_fn : run_network(inputs, viewdirs, network_fn,\n",
    "                                                                embed_fn=encode,\n",
    "                                                                embeddirs_fn=encode_viewdirs,\n",
    "                                                                netchunk=chunksize)\n",
    "\n",
    "  # Checkpoints loading\n",
    "  os.makedirs(os.path.join(basedir, expname), exist_ok=True)\n",
    "  ckpts = [os.path.join(basedir, expname, f) for f in sorted(os.listdir(os.path.join(basedir, expname))) if 'tar' in f]\n",
    "  print(\"Found ckpts\", ckpts)\n",
    "\n",
    "  if len(ckpts) > 0:\n",
    "    ckpt_path = ckpts[-1]\n",
    "    print('testing from', ckpt_path)\n",
    "    ckpt = torch.load(ckpt_path)\n",
    "\n",
    "    global start\n",
    "    start = ckpt['global_step']\n",
    "    #optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "\n",
    "    # Load model\n",
    "    model.load_state_dict(ckpt['coarse_model_dict'])\n",
    "    if fine_model is not None:\n",
    "        fine_model.load_state_dict(ckpt['fine_model_dict'])\n",
    "\n",
    "  render_kwargs_train = {\n",
    "        'network_query_fn' : network_query_fn,\n",
    "        'perturb' : perturb,\n",
    "        'N_importance' : N_importance,\n",
    "        'network_fine' : fine_model,\n",
    "        'N_samples' : n_samples,\n",
    "        'network_fn' : model,\n",
    "        'use_viewdirs' : use_viewdirs,\n",
    "        'white_bkgd' : False,\n",
    "        'raw_noise_std' : 0.,\n",
    "    }\n",
    "  \n",
    "  # NDC only good for LLFF-style forward facing data\n",
    "  if not use_ndc:\n",
    "    print('Not ndc!')\n",
    "    render_kwargs_train['ndc'] = False\n",
    "    #render_kwargs_train['lindisp'] = args.lindisp\n",
    "\n",
    "  render_kwargs_test = {k : render_kwargs_train[k] for k in render_kwargs_train}\n",
    "  render_kwargs_test['perturb'] = False\n",
    "  render_kwargs_test['raw_noise_std'] = 0.\n",
    "\n",
    "  return render_kwargs_train, render_kwargs_test, start, model_params#, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_rays(ray_batch,\n",
    "                network_fn,\n",
    "                network_query_fn,\n",
    "                N_samples,\n",
    "                retraw=False,\n",
    "                lindisp=False,\n",
    "                perturb=0.,\n",
    "                N_importance=0,\n",
    "                network_fine=None,\n",
    "                white_bkgd=False,\n",
    "                raw_noise_std=0.,\n",
    "                verbose=False,\n",
    "                pytest=False):\n",
    "    \"\"\"Volumetric rendering.\n",
    "    Args:\n",
    "      ray_batch: array of shape [batch_size, ...]. All information necessary\n",
    "        for sampling along a ray, including: ray origin, ray direction, min\n",
    "        dist, max dist, and unit-magnitude viewing direction.\n",
    "      network_fn: function. Model for predicting RGB and density at each point\n",
    "        in space.\n",
    "      network_query_fn: function used for passing queries to network_fn.\n",
    "      N_samples: int. Number of different times to sample along each ray.\n",
    "      retraw: bool. If True, include model's raw, unprocessed predictions.\n",
    "      lindisp: bool. If True, sample linearly in inverse depth rather than in depth.\n",
    "      perturb: float, 0 or 1. If non-zero, each ray is sampled at stratified\n",
    "        random points in time.\n",
    "      N_importance: int. Number of additional times to sample along each ray.\n",
    "        These samples are only passed to network_fine.\n",
    "      network_fine: \"fine\" network with same spec as network_fn.\n",
    "      white_bkgd: bool. If True, assume a white background.\n",
    "      raw_noise_std: ...\n",
    "      verbose: bool. If True, print more debugging info.\n",
    "    Returns:\n",
    "      rgb_map: [num_rays, 3]. Estimated RGB color of a ray. Comes from fine model.\n",
    "      disp_map: [num_rays]. Disparity map. 1 / depth.\n",
    "      acc_map: [num_rays]. Accumulated opacity along each ray. Comes from fine model.\n",
    "      raw: [num_rays, num_samples, 4]. Raw predictions from model.\n",
    "      rgb0: See rgb_map. Output for coarse model.\n",
    "      disp0: See disp_map. Output for coarse model.\n",
    "      acc0: See acc_map. Output for coarse model.\n",
    "      z_std: [num_rays]. Standard deviation of distances along ray for each\n",
    "        sample.\n",
    "    \"\"\"\n",
    "    N_rays = ray_batch.shape[0]\n",
    "    rays_o, rays_d = ray_batch[:,0:3], ray_batch[:,3:6] # [N_rays, 3] each\n",
    "    viewdirs = ray_batch[:,-3:] if ray_batch.shape[-1] > 8 else None\n",
    "    bounds = torch.reshape(ray_batch[...,6:8], [-1,1,2])\n",
    "    near, far = bounds[...,0], bounds[...,1] # [-1,1]\n",
    "\n",
    "    t_vals = torch.linspace(0., 1., steps=N_samples).to(device)\n",
    "    if not lindisp:\n",
    "        z_vals = near * (1.-t_vals) + far * (t_vals)\n",
    "    else:\n",
    "        z_vals = 1./(1./near * (1.-t_vals) + 1./far * (t_vals))\n",
    "\n",
    "    z_vals = z_vals.expand([N_rays, N_samples])\n",
    "\n",
    "    if perturb > 0.:\n",
    "        # get intervals between samples\n",
    "        mids = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n",
    "        upper = torch.cat([mids, z_vals[...,-1:]], -1)\n",
    "        lower = torch.cat([z_vals[...,:1], mids], -1)\n",
    "        # stratified samples in those intervals\n",
    "        t_rand = torch.rand(z_vals.shape)\n",
    "\n",
    "        # Pytest, overwrite u with numpy's fixed random numbers\n",
    "        if pytest:\n",
    "            np.random.seed(0)\n",
    "            t_rand = np.random.rand(*list(z_vals.shape))\n",
    "            t_rand = torch.Tensor(t_rand)\n",
    "\n",
    "        z_vals = lower + (upper - lower) * t_rand\n",
    "\n",
    "    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples, 3]\n",
    "\n",
    "\n",
    "#     raw = run_network(pts)\n",
    "    raw = network_query_fn(pts, viewdirs, network_fn)\n",
    "    #print(raw.shape)\n",
    "    #rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd)#, pytest=pytest)\n",
    "    rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd)#, pytest=pytest)\n",
    "\n",
    "    if N_importance > 0:\n",
    "\n",
    "        rgb_map_0, disp_map_0, acc_map_0 = rgb_map, disp_map, acc_map\n",
    "\n",
    "        z_vals_mid = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n",
    "        z_samples = sample_pdf(z_vals_mid, weights[...,1:-1], N_importance, perturb=False)#, pytest=pytest)\n",
    "        z_samples = z_samples.detach()\n",
    "\n",
    "        z_vals, _ = torch.sort(torch.cat([z_vals, z_samples], -1), -1)\n",
    "        pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples + N_importance, 3]\n",
    "\n",
    "        run_fn = network_fn if network_fine is None else network_fine\n",
    "#         raw = run_network(pts, fn=run_fn)\n",
    "        raw = network_query_fn(pts, viewdirs, run_fn)\n",
    "\n",
    "        rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd)#, pytest=pytest)\n",
    "\n",
    "    ret = {'rgb_map' : rgb_map, 'disp_map' : disp_map, 'acc_map' : acc_map}\n",
    "    if retraw:\n",
    "        ret['raw'] = raw\n",
    "    if N_importance > 0:\n",
    "        ret['rgb0'] = rgb_map_0\n",
    "        ret['disp0'] = disp_map_0\n",
    "        ret['acc0'] = acc_map_0\n",
    "        ret['z_std'] = torch.std(z_samples, dim=-1, unbiased=False)  # [N_rays]\n",
    "    '''\n",
    "    for k in ret:\n",
    "        if (torch.isnan(ret[k]).any() or torch.isinf(ret[k]).any()) and DEBUG:\n",
    "            print(f\"! [Numerical Error] {k} contains nan or inf.\")\n",
    "    '''\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    K = None\n",
    "    data_dir = \"H:\\\\NeRF-NTUE-project\\\\fern\"\n",
    "    images, poses, bds, render_poses, i_test = load_llff_data(data_dir)\n",
    "    focal = poses[0, 2, 4]\n",
    "    height, width = poses[0, :2, -1]\n",
    "    height, width = int(height), int(width)\n",
    "    hwf = [height, width, focal]\n",
    "    poses = poses[:, :3, :4]\n",
    "\n",
    "    print(f'Images shape: {images.shape}')\n",
    "    print(f'Poses shape: {poses.shape}')\n",
    "    print(f'Focal: {focal}')\n",
    "\n",
    "    if not isinstance(i_test, list):\n",
    "        i_test = [i_test]\n",
    "\n",
    "    if llff_hold > 0:\n",
    "        print(f'Set hold out frequency: {llff_hold}')\n",
    "        i_test = np.arange(images.shape[0])[::llff_hold]\n",
    "    i_val = i_test\n",
    "    n_training = np.array([i for i in np.arange(int(images.shape[0])) if (i not in i_test and i not in i_val)])\n",
    "\n",
    "    if use_ndc:\n",
    "        near, far = 0., 1.\n",
    "    else:\n",
    "        near = np.ndarray.min(bds) *.9\n",
    "        far = np.ndarray.max(bds) * 1.\n",
    "\n",
    "    print(f'Define near far: {near}, {far}')\n",
    "\n",
    "    print(f'Train views are: {n_training}')\n",
    "    print(f'Val views are: {i_val}')\n",
    "    print(f'Test views are: {i_test}')\n",
    "\n",
    "    '''\n",
    "    plt.imshow(images[i_val[0]])\n",
    "    print('Pose')\n",
    "    print(poses[i_val[0]])\n",
    "    '''\n",
    "    \n",
    "    if K is None:\n",
    "        K = np.array([\n",
    "            [focal, 0, 0.5*width],\n",
    "            [0, focal, 0.5*height],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "\n",
    "    if render_test:\n",
    "        render_poses = np.array(poses[i_test])\n",
    "\n",
    "    os.makedirs(os.path.join(basedir, expname), exist_ok=True)\n",
    "    # Create nerf model\n",
    "    render_kwargs_train, render_kwargs_test, start, grad_vars = init_models()\n",
    "    global_step = start\n",
    "\n",
    "    bds_dict = {\n",
    "        'near' : near,\n",
    "        'far' : far,\n",
    "    }\n",
    "    render_kwargs_train.update(bds_dict)\n",
    "    render_kwargs_test.update(bds_dict)\n",
    "\n",
    "    # Move testing data to GPU\n",
    "    render_poses = torch.Tensor(render_poses).to(device)\n",
    "\n",
    "    # Short circuit if only rendering out from trained model\n",
    "    #if args.render_only:\n",
    "    if True:\n",
    "        print('RENDER ONLY')\n",
    "        with torch.no_grad():\n",
    "            if render_test:\n",
    "                # render_test switches to test poses\n",
    "                images = images[i_test]\n",
    "            else:\n",
    "                # Default is smoother render_poses path\n",
    "                images = None\n",
    "\n",
    "            testsavedir = os.path.join(basedir, expname, 'renderonly_{}_{:06d}'.format('test' if render_test else 'path', start))\n",
    "            os.makedirs(testsavedir, exist_ok=True)\n",
    "            print('test poses shape', render_poses.shape)\n",
    "\n",
    "            #rgbs, _ = render_path(render_poses, hwf, K, args.chunk, render_kwargs_test, gt_imgs=images, savedir=testsavedir, render_factor=args.render_factor)\n",
    "            rgbs, _ = render_path(render_poses, hwf, K, chunksize, render_kwargs_test, gt_imgs=images, savedir=testsavedir, render_factor=0)\n",
    "            print('Done rendering', testsavedir)\n",
    "            #imageio.mimwrite(os.path.join(testsavedir, 'video.mp4'), to8b(rgbs), fps=30, quality=8)\n",
    "\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image data (20, 378, 504, 3) [378.         504.         407.56579161]\n",
      "Loaded H:\\NeRF-NTUE-project\\fern 16.985296178676084 80.00209740336334\n",
      "recentered (3, 5)\n",
      "[[ 1.0000000e+00  0.0000000e+00  0.0000000e+00  1.4901161e-09]\n",
      " [ 0.0000000e+00  1.0000000e+00 -1.8730975e-09 -9.6857544e-09]\n",
      " [-0.0000000e+00  1.8730975e-09  1.0000000e+00  0.0000000e+00]]\n",
      "Data:\n",
      "(20, 3, 5) (20, 378, 504, 3) (20, 2)\n",
      "HOLDOUT view is 12\n",
      "Images shape: (20, 378, 504, 3)\n",
      "Poses shape: (20, 3, 4)\n",
      "Focal: 407.5657958984375\n",
      "Set hold out frequency: 8\n",
      "Define near far: 0.0, 1.0\n",
      "Train views are: [ 1  2  3  4  5  6  7  9 10 11 12 13 14 15 17 18 19]\n",
      "Val views are: [ 0  8 16]\n",
      "Test views are: [ 0  8 16]\n",
      "Found ckpts ['.\\\\logs\\\\fern_test\\\\010000.tar']\n",
      "testing from .\\logs\\fern_test\\010000.tar\n",
      "RENDER ONLY\n",
      "test poses shape torch.Size([120, 3, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/120 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.004998445510864258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/120 [00:58<1:56:34, 58.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([378, 504, 3]) torch.Size([378, 504])\n",
      "1 58.78027057647705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/120 [01:57<1:55:24, 58.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 58.61058592796326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 3/120 [02:54<1:53:24, 58.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 57.532970666885376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4/120 [03:51<1:51:36, 57.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 57.06911301612854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 5/120 [04:48<1:50:04, 57.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 56.89284586906433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 6/120 [05:47<1:49:53, 57.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 58.640366554260254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 7/120 [06:44<1:48:32, 57.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 57.20531129837036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 8/120 [07:40<1:46:36, 57.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 56.002527952194214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 8/120 [07:58<1:51:39, 59.82s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m#torch.set_default_tensor_type('torch.cuda.FloatTensor')\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 84\u001b[0m, in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest poses shape\u001b[39m\u001b[38;5;124m'\u001b[39m, render_poses\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m#rgbs, _ = render_path(render_poses, hwf, K, args.chunk, render_kwargs_test, gt_imgs=images, savedir=testsavedir, render_factor=args.render_factor)\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m rgbs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrender_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrender_poses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhwf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_kwargs_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_imgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msavedir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtestsavedir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone rendering\u001b[39m\u001b[38;5;124m'\u001b[39m, testsavedir)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m#imageio.mimwrite(os.path.join(testsavedir, 'video.mp4'), to8b(rgbs), fps=30, quality=8)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 18\u001b[0m, in \u001b[0;36mrender_path\u001b[1;34m(render_poses, hwf, K, chunk, render_kwargs, gt_imgs, savedir, render_factor)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(i, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t)\n\u001b[0;32m     17\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 18\u001b[0m rgb, disp, acc, _ \u001b[38;5;241m=\u001b[39m render(H, W, K, chunk\u001b[38;5;241m=\u001b[39mchunk, c2w\u001b[38;5;241m=\u001b[39mc2w[:\u001b[38;5;241m3\u001b[39m,:\u001b[38;5;241m4\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrender_kwargs)\n\u001b[0;32m     19\u001b[0m rgbs\u001b[38;5;241m.\u001b[39mappend(rgb\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     20\u001b[0m disps\u001b[38;5;241m.\u001b[39mappend(disp\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "Cell \u001b[1;32mIn[17], line 60\u001b[0m, in \u001b[0;36mrender\u001b[1;34m(H, W, K, chunk, rays, c2w, use_ndc, near, far, use_viewdirs, c2w_staticcam, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     rays \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([rays, viewdirs], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Render and reshape\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m all_ret \u001b[38;5;241m=\u001b[39m batchify_rays(rays, chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m all_ret:\n\u001b[0;32m     62\u001b[0m     k_sh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(sh[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(all_ret[k]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m, in \u001b[0;36mbatchify_rays\u001b[1;34m(rays_flat, chunk, **kwargs)\u001b[0m\n\u001b[0;32m      5\u001b[0m all_ret \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, rays_flat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], chunk):\n\u001b[1;32m----> 7\u001b[0m     ret \u001b[38;5;241m=\u001b[39m render_rays(rays_flat[i:i\u001b[38;5;241m+\u001b[39mchunk], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m ret:\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_ret:\n",
      "Cell \u001b[1;32mIn[21], line 96\u001b[0m, in \u001b[0;36mrender_rays\u001b[1;34m(ray_batch, network_fn, network_query_fn, N_samples, retraw, lindisp, perturb, N_importance, network_fine, white_bkgd, raw_noise_std, verbose, pytest)\u001b[0m\n\u001b[0;32m     94\u001b[0m         run_fn \u001b[38;5;241m=\u001b[39m network_fn \u001b[38;5;28;01mif\u001b[39;00m network_fine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m network_fine\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m#         raw = run_network(pts, fn=run_fn)\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m         raw \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork_query_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mviewdirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m         rgb_map, disp_map, acc_map, weights, depth_map \u001b[38;5;241m=\u001b[39m raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd)\u001b[38;5;66;03m#, pytest=pytest)\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     ret \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb_map\u001b[39m\u001b[38;5;124m'\u001b[39m : rgb_map, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp_map\u001b[39m\u001b[38;5;124m'\u001b[39m : disp_map, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc_map\u001b[39m\u001b[38;5;124m'\u001b[39m : acc_map}\n",
      "Cell \u001b[1;32mIn[20], line 34\u001b[0m, in \u001b[0;36minit_models.<locals>.<lambda>\u001b[1;34m(inputs, viewdirs, network_fn)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m   fine_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m network_query_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m inputs, viewdirs, network_fn : \u001b[43mrun_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mviewdirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43membed_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43membeddirs_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_viewdirs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mnetchunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Checkpoints loading\u001b[39;00m\n\u001b[0;32m     40\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(basedir, expname), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[16], line 15\u001b[0m, in \u001b[0;36mrun_network\u001b[1;34m(inputs, viewdirs, fn, embed_fn, embeddirs_fn, netchunk)\u001b[0m\n\u001b[0;32m     12\u001b[0m     embedded_dirs \u001b[38;5;241m=\u001b[39m embeddirs_fn(input_dirs_flat)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m#embedded = torch.cat([embedded, embedded_dirs], -1)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m outputs_flat \u001b[38;5;241m=\u001b[39m \u001b[43mbatchify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mviewdirs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedded_dirs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(outputs_flat, \u001b[38;5;28mlist\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m [outputs_flat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m, in \u001b[0;36mbatchify.<locals>.ret\u001b[1;34m(inputs, viewdirs)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mret\u001b[39m(inputs, viewdirs):\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([fn(inputs[i:i\u001b[38;5;241m+\u001b[39mchunk], viewdirs[i:i\u001b[38;5;241m+\u001b[39mchunk]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], chunk)], \u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mret\u001b[39m(inputs, viewdirs):\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mchunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mviewdirs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mchunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], chunk)], \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pro88\\anaconda3\\envs\\nerf_ntue\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pro88\\anaconda3\\envs\\nerf_ntue\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mh:\\NeRF-NTUE-project\\model.py:98\u001b[0m, in \u001b[0;36mNeRF.forward\u001b[1;34m(self, x, viewdirs, sigma_only)\u001b[0m\n\u001b[0;32m     96\u001b[0m x_input \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m---> 98\u001b[0m   x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     99\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip:\n\u001b[0;32m    100\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, x_input], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pro88\\anaconda3\\envs\\nerf_ntue\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pro88\\anaconda3\\envs\\nerf_ntue\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pro88\\anaconda3\\envs\\nerf_ntue\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    #torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training session\n",
    "for _ in range(1):\n",
    "  model, fine_model, encode, encode_viewdirs= init_models()\n",
    "  success, val_psnrs = test()\n",
    "  #if success and val_psnrs[-1] >= warmup_min_fitness:\n",
    "  if success:\n",
    "    print('Testing successful!')\n",
    "    break\n",
    "\n",
    "print('')\n",
    "print(f'Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerf_ntue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
