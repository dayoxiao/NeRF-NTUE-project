{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "import time\n",
    "import imageio\n",
    "import cv2\n",
    "\n",
    "from model import *\n",
    "from rays_util import *\n",
    "from render import *\n",
    "from load_llff import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(fn, chunk):\n",
    "    \"\"\"\n",
    "    Constructs a version of 'fn' that applies to smaller batches.\n",
    "    \"\"\"\n",
    "    if chunk is None:\n",
    "        return fn\n",
    "    def ret(inputs, viewdirs):\n",
    "        return torch.cat([fn(inputs[i:i+chunk], viewdirs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_rays(rays_flat, chunk=1024*32, **kwargs):\n",
    "    \"\"\"\n",
    "    Render rays in smaller minibatches to avoid OOM.\n",
    "    \"\"\"\n",
    "    all_ret = {}\n",
    "    for i in range(0, rays_flat.shape[0], chunk):\n",
    "        ret = render_rays(rays_flat[i:i+chunk], **kwargs)\n",
    "        for k in ret:\n",
    "            if k not in all_ret:\n",
    "                all_ret[k] = []\n",
    "            all_ret[k].append(ret[k])\n",
    "\n",
    "    all_ret = {k : torch.cat(all_ret[k], 0) for k in all_ret}\n",
    "    return all_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_network(inputs, viewdirs, fn, embed_fn, embeddirs_fn, netchunk=1024*64):\n",
    "    \"\"\"\n",
    "    Prepares inputs and applies network 'fn'.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs_flat = torch.reshape(inputs, [-1, inputs.shape[-1]])\n",
    "    embedded = embed_fn(inputs_flat)\n",
    "    \n",
    "    if viewdirs is not None:\n",
    "        input_dirs = viewdirs[:,None].expand(inputs.shape)\n",
    "        input_dirs_flat = torch.reshape(input_dirs, [-1, input_dirs.shape[-1]])\n",
    "        embedded_dirs = embeddirs_fn(input_dirs_flat)\n",
    "\n",
    "    outputs_flat = batchify(fn, netchunk)(embedded, viewdirs=embedded_dirs)\n",
    "    outputs = torch.reshape(outputs_flat, list(inputs.shape[:-1]) + [outputs_flat.shape[-1]])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(H, W, K, chunk=1024*32, rays=None, c2w=None, use_ndc=True,\n",
    "                  near=0., far=1.,\n",
    "                  use_viewdirs=False, c2w_staticcam=None,\n",
    "                  **kwargs):\n",
    "    \"\"\"Render rays\n",
    "    Args:\n",
    "      H: int. Height of image in pixels.\n",
    "      W: int. Width of image in pixels.\n",
    "      focal: float. Focal length of pinhole camera.\n",
    "      chunk: int. Maximum number of rays to process simultaneously. Used to\n",
    "        control maximum memory usage. Does not affect final results.\n",
    "      rays: array of shape [2, batch_size, 3]. Ray origin and direction for\n",
    "        each example in batch.\n",
    "      c2w: array of shape [3, 4]. Camera-to-world transformation matrix.\n",
    "      ndc: bool. If True, represent ray origin, direction in NDC coordinates.\n",
    "      near: float or array of shape [batch_size]. Nearest distance for a ray.\n",
    "      far: float or array of shape [batch_size]. Farthest distance for a ray.\n",
    "      use_viewdirs: bool. If True, use viewing direction of a point in space in model.\n",
    "      c2w_staticcam: array of shape [3, 4]. If not None, use this transformation matrix for \n",
    "       camera while using other c2w argument for viewing directions.\n",
    "    Returns:\n",
    "      rgb_map: [batch_size, 3]. Predicted RGB values for rays.\n",
    "      disp_map: [batch_size]. Disparity map. Inverse of depth.\n",
    "      acc_map: [batch_size]. Accumulated opacity (alpha) along a ray.\n",
    "      extras: dict with everything returned by render_rays().\n",
    "    \"\"\"\n",
    "    f = K[0][0]\n",
    "\n",
    "    if c2w is not None:\n",
    "        # special case to render full image\n",
    "        rays_o, rays_d = get_rays(H, W, f, c2w)\n",
    "    else:\n",
    "        # use provided ray batch\n",
    "        rays_o, rays_d = rays\n",
    "\n",
    "    if use_viewdirs:\n",
    "        # provide ray directions as input\n",
    "        viewdirs = rays_d\n",
    "        if c2w_staticcam is not None:\n",
    "            # special case to visualize effect of viewdirs\n",
    "            rays_o, rays_d = get_rays(H, W, f, c2w_staticcam)\n",
    "        viewdirs = viewdirs / torch.norm(viewdirs, dim=-1, keepdim=True)\n",
    "        viewdirs = torch.reshape(viewdirs, [-1,3]).float()\n",
    "\n",
    "    sh = rays_d.shape # [..., 3]\n",
    "    if use_ndc:\n",
    "        # for forward facing scenes\n",
    "        rays_o, rays_d = ndc_rays(H, W, f, 1., rays_o, rays_d)\n",
    "\n",
    "    # Create ray batch\n",
    "    rays_o = torch.reshape(rays_o, [-1,3]).float()\n",
    "    rays_d = torch.reshape(rays_d, [-1,3]).float()\n",
    "\n",
    "    near, far = near * torch.ones_like(rays_d[...,:1]), far * torch.ones_like(rays_d[...,:1])\n",
    "    rays = torch.cat([rays_o, rays_d, near, far], -1)\n",
    "    if use_viewdirs:\n",
    "        rays = torch.cat([rays, viewdirs], -1)\n",
    "\n",
    "    # Render and reshape\n",
    "    all_ret = batchify_rays(rays, chunk, **kwargs)\n",
    "    for k in all_ret:\n",
    "        k_sh = list(sh[:-1]) + list(all_ret[k].shape[1:])\n",
    "        all_ret[k] = torch.reshape(all_ret[k], k_sh)\n",
    "\n",
    "    k_extract = ['rgb_map', 'disp_map', 'acc_map']\n",
    "    ret_list = [all_ret[k] for k in k_extract]\n",
    "    ret_dict = {k : all_ret[k] for k in all_ret if k not in k_extract}\n",
    "    return ret_list + [ret_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_path(render_poses, hwf, K, chunk, render_kwargs, gt_imgs=None, savedir=None, render_factor=0):\n",
    "\n",
    "    H, W, focal = hwf\n",
    "\n",
    "    if render_factor!=0:\n",
    "        # Render downsampled for speed\n",
    "        H = H//render_factor\n",
    "        W = W//render_factor\n",
    "        focal = focal/render_factor\n",
    "\n",
    "    rgbs = []\n",
    "    disps = []\n",
    "    depths = []\n",
    "\n",
    "    t = time.time()\n",
    "    for i, c2w in enumerate(tqdm(render_poses)):\n",
    "        print(i, time.time() - t)\n",
    "        t = time.time()\n",
    "        #rgb, disp, acc, _ = render(H, W, K, chunk=chunk, c2w=c2w[:3,:4], **render_kwargs)\n",
    "        rgb, disp, acc, extras = render(H, W, K, chunk=chunk, c2w=c2w[:3,:4], **render_kwargs)\n",
    "        rgbs.append(rgb.cpu().numpy())\n",
    "        disps.append(disp.cpu().numpy())\n",
    "        depths.append(extras['depth_map'].cpu().numpy())\n",
    "        if i==0:\n",
    "            print(rgb.shape, disp.shape)\n",
    "\n",
    "        \"\"\"\n",
    "        if gt_imgs is not None and render_factor==0:\n",
    "            p = -10. * np.log10(np.mean(np.square(rgb.cpu().numpy() - gt_imgs[i])))\n",
    "            print(p)\n",
    "        \"\"\"\n",
    "\n",
    "        if savedir is not None:\n",
    "            rgb8 = to8b(rgbs[-1])\n",
    "            filename = os.path.join(savedir, '{:03d}.png'.format(i))\n",
    "            imageio.imwrite(filename, rgb8)\n",
    "\n",
    "\n",
    "    rgbs = np.stack(rgbs, 0)\n",
    "    disps = np.stack(disps, 0)\n",
    "    depths = np.stack(depths, 0)\n",
    "\n",
    "    return rgbs, disps, depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "# Model\n",
    "d_filter = 256          # Dimensions of linear layer filters\n",
    "n_layers = 8            # Number of layers in network bottleneck\n",
    "skip = [4]              # Layers at which to apply input residual\n",
    "use_fine_model = True   # If set, creates a fine model\n",
    "d_filter_fine = 256     # Dimensions of linear layer filters of fine network\n",
    "n_layers_fine = 8       # Number of layers in fine network bottleneck\n",
    "\n",
    "# Stratified sampling\n",
    "n_samples = 64          # Number of spatial samples per ray\n",
    "perturb = False         # If set, applies noise to sample positions\n",
    "inverse_depth = False    # If set, samples points linearly in inverse depth\n",
    "\n",
    "# Hierarchical sampling\n",
    "N_importance = 64       # Number of samples per ray\n",
    "perturb_hierarchical = False  # If set, applies noise to sample positions\n",
    "\n",
    "# Encoder\n",
    "d_input = 3             # Number of input dimensions\n",
    "n_freqs = 10            # Number of encoding functions for samples\n",
    "log_sampling = True     # If set, frequencies scale in log space\n",
    "use_viewdirs = True     # If set, use view direction as input\n",
    "n_freqs_views = 4       # Number of encoding functions for views\n",
    "\n",
    "# Training\n",
    "batch_size = 1024       # Number of rays per gradient step (power of 2)\n",
    "chunksize = 1024        # Modify as needed to fit in GPU memory\n",
    "\n",
    "# LLFF & Dataloading\n",
    "start = 0                   # Starting iteration, 0 by default\n",
    "basedir = \".\\\\logs\"         # Base directory for logs and ckpts\n",
    "expname = \"ParrotnPlate\"    # Custom experiment name\n",
    "data_dir = \".\\\\nerf_sample_parrotnPlate\"         # Input data directory\n",
    "dataset_type = \"llff\"       # Dataset Type\n",
    "factor = 4                  # Load down scaled image, \n",
    "                            # NEEDS to be create manually inside the folder such as, image_4\n",
    "render_factor = 2           # Downsampling factor for fast rendering\n",
    "spherify = True             # Set if it's for 360 inward scenes\n",
    "llff_hold = 8               # if set, take image 1/N as test set\n",
    "use_ndc = False             # use ndc for forward facing scenes\n",
    "render_test = False         # render test set instead of custom poses\n",
    "\n",
    "# Function kwargs\n",
    "stratified_sampling_kwargs = {\n",
    "    'n_samples': n_samples,\n",
    "    'perturb': perturb,\n",
    "    'inverse_depth': inverse_depth\n",
    "}\n",
    "hierarchical_sampling_kwargs = {\n",
    "    'perturb': perturb\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_models():\n",
    "  \"\"\"\n",
    "  Initialize models, encoders, and optimizer for training.\n",
    "  \"\"\"\n",
    "\n",
    "  # Encoders\n",
    "  encoder = PositionalEncoder(d_input, n_freqs, log_sampling=log_sampling)\n",
    "  encode = lambda x: encoder(x)\n",
    "\n",
    "  # View direction encoders\n",
    "  if use_viewdirs:\n",
    "    encoder_viewdirs = PositionalEncoder(d_input, n_freqs_views,\n",
    "                                        log_sampling=log_sampling)\n",
    "    encode_viewdirs = lambda x: encoder_viewdirs(x)\n",
    "    d_viewdirs = encoder_viewdirs.d_output\n",
    "    test_dir = d_viewdirs\n",
    "  else:\n",
    "    encode_viewdirs = None\n",
    "    d_viewdirs = None\n",
    "\n",
    "  # Models\n",
    "  model = NeRF(encoder.d_output, n_layers=n_layers, d_filter=d_filter, skip=skip,\n",
    "              d_viewdirs=d_viewdirs)\n",
    "  model.to(device)\n",
    "  model_params = list(model.parameters())\n",
    "  if use_fine_model:\n",
    "    fine_model = NeRF(encoder.d_output, n_layers=n_layers, d_filter=d_filter, skip=skip,\n",
    "                      d_viewdirs=d_viewdirs)\n",
    "    fine_model.to(device)\n",
    "    model_params = model_params + list(fine_model.parameters())\n",
    "  else:\n",
    "    fine_model = None\n",
    "  \n",
    "  network_query_fn = lambda inputs, viewdirs, network_fn : run_network(inputs, viewdirs, network_fn,\n",
    "                                                                embed_fn=encode,\n",
    "                                                                embeddirs_fn=encode_viewdirs,\n",
    "                                                                netchunk=chunksize)\n",
    "\n",
    "  # Checkpoints loading\n",
    "  os.makedirs(os.path.join(basedir, expname), exist_ok=True)\n",
    "  ckpts = [os.path.join(basedir, expname, f) for f in sorted(os.listdir(os.path.join(basedir, expname))) if 'tar' in f]\n",
    "  print(\"Found ckpts\", ckpts)\n",
    "\n",
    "  if len(ckpts) > 0:\n",
    "    ckpt_path = ckpts[-1]\n",
    "    print('testing from', ckpt_path)\n",
    "    ckpt = torch.load(ckpt_path)\n",
    "\n",
    "    global start\n",
    "    start = ckpt['global_step']\n",
    "    #optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "\n",
    "    # Load model\n",
    "    model.load_state_dict(ckpt['coarse_model_dict'])\n",
    "    if fine_model is not None:\n",
    "        fine_model.load_state_dict(ckpt['fine_model_dict'])\n",
    "\n",
    "  render_kwargs_train = {\n",
    "        'network_query_fn' : network_query_fn,\n",
    "        'perturb' : perturb,\n",
    "        'N_importance' : N_importance,\n",
    "        'network_fine' : fine_model,\n",
    "        'N_samples' : n_samples,\n",
    "        'network_fn' : model,\n",
    "        'use_viewdirs' : use_viewdirs,\n",
    "        'white_bkgd' : False,\n",
    "        'raw_noise_std' : 0.,\n",
    "    }\n",
    "  \n",
    "  # NDC only good for LLFF-style forward facing data\n",
    "  if not use_ndc:\n",
    "    print('Not ndc!')\n",
    "    render_kwargs_train['use_ndc'] = False\n",
    "    render_kwargs_train['lindisp'] = inverse_depth\n",
    "\n",
    "  render_kwargs_test = {k : render_kwargs_train[k] for k in render_kwargs_train}\n",
    "  render_kwargs_test['perturb'] = False\n",
    "  render_kwargs_test['raw_noise_std'] = 0.\n",
    "\n",
    "  return render_kwargs_train, render_kwargs_test, start, model_params#, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_rays(ray_batch,\n",
    "                network_fn,\n",
    "                network_query_fn,\n",
    "                N_samples,\n",
    "                retraw=False,\n",
    "                lindisp=False,\n",
    "                perturb=0.,\n",
    "                N_importance=0,\n",
    "                network_fine=None,\n",
    "                white_bkgd=False,\n",
    "                raw_noise_std=0.,\n",
    "                verbose=False,\n",
    "                pytest=False):\n",
    "    \"\"\"Volumetric rendering.\n",
    "    Args:\n",
    "      ray_batch: array of shape [batch_size, ...]. All information necessary\n",
    "        for sampling along a ray, including: ray origin, ray direction, min\n",
    "        dist, max dist, and unit-magnitude viewing direction.\n",
    "      network_fn: function. Model for predicting RGB and density at each point\n",
    "        in space.\n",
    "      network_query_fn: function used for passing queries to network_fn.\n",
    "      N_samples: int. Number of different times to sample along each ray.\n",
    "      retraw: bool. If True, include model's raw, unprocessed predictions.\n",
    "      lindisp: bool. If True, sample linearly in inverse depth rather than in depth.\n",
    "      perturb: float, 0 or 1. If non-zero, each ray is sampled at stratified\n",
    "        random points in time.\n",
    "      N_importance: int. Number of additional times to sample along each ray.\n",
    "        These samples are only passed to network_fine.\n",
    "      network_fine: \"fine\" network with same spec as network_fn.\n",
    "      white_bkgd: bool. If True, assume a white background.\n",
    "      raw_noise_std: ...\n",
    "      verbose: bool. If True, print more debugging info.\n",
    "    Returns:\n",
    "      rgb_map: [num_rays, 3]. Estimated RGB color of a ray. Comes from fine model.\n",
    "      disp_map: [num_rays]. Disparity map. 1 / depth.\n",
    "      acc_map: [num_rays]. Accumulated opacity along each ray. Comes from fine model.\n",
    "      raw: [num_rays, num_samples, 4]. Raw predictions from model.\n",
    "      rgb0: See rgb_map. Output for coarse model.\n",
    "      disp0: See disp_map. Output for coarse model.\n",
    "      acc0: See acc_map. Output for coarse model.\n",
    "      z_std: [num_rays]. Standard deviation of distances along ray for each\n",
    "        sample.\n",
    "    \"\"\"\n",
    "    N_rays = ray_batch.shape[0]\n",
    "    rays_o, rays_d = ray_batch[:,0:3], ray_batch[:,3:6] # [N_rays, 3] each\n",
    "    viewdirs = ray_batch[:,-3:] if ray_batch.shape[-1] > 8 else None\n",
    "    bounds = torch.reshape(ray_batch[...,6:8], [-1,1,2])\n",
    "    near, far = bounds[...,0], bounds[...,1] # [-1,1]\n",
    "\n",
    "    t_vals = torch.linspace(0., 1., steps=N_samples).to(device)\n",
    "    if not lindisp:\n",
    "        z_vals = near * (1.-t_vals) + far * (t_vals)\n",
    "    else:\n",
    "        z_vals = 1./(1./near * (1.-t_vals) + 1./far * (t_vals))\n",
    "\n",
    "    z_vals = z_vals.expand([N_rays, N_samples])\n",
    "\n",
    "    if perturb > 0.:\n",
    "        # get intervals between samples\n",
    "        mids = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n",
    "        upper = torch.cat([mids, z_vals[...,-1:]], -1)\n",
    "        lower = torch.cat([z_vals[...,:1], mids], -1)\n",
    "        # stratified samples in those intervals\n",
    "        t_rand = torch.rand(z_vals.shape)\n",
    "\n",
    "        # Pytest, overwrite u with numpy's fixed random numbers\n",
    "        if pytest:\n",
    "            np.random.seed(0)\n",
    "            t_rand = np.random.rand(*list(z_vals.shape))\n",
    "            t_rand = torch.Tensor(t_rand)\n",
    "\n",
    "        z_vals = lower + (upper - lower) * t_rand\n",
    "\n",
    "    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples, 3]\n",
    "\n",
    "\n",
    "#     raw = run_network(pts)\n",
    "    raw = network_query_fn(pts, viewdirs, network_fn)\n",
    "    #print(raw.shape)\n",
    "    #rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd)#, pytest=pytest)\n",
    "    rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd)#, pytest=pytest)\n",
    "\n",
    "    if N_importance > 0:\n",
    "\n",
    "        rgb_map_0, disp_map_0, acc_map_0 = rgb_map, disp_map, acc_map\n",
    "\n",
    "        z_vals_mid = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n",
    "        z_samples = sample_pdf(z_vals_mid, weights[...,1:-1], N_importance, perturb=False)#, pytest=pytest)\n",
    "        z_samples = z_samples.detach()\n",
    "\n",
    "        z_vals, _ = torch.sort(torch.cat([z_vals, z_samples], -1), -1)\n",
    "        pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples + N_importance, 3]\n",
    "\n",
    "        run_fn = network_fn if network_fine is None else network_fine\n",
    "#         raw = run_network(pts, fn=run_fn)\n",
    "        raw = network_query_fn(pts, viewdirs, run_fn)\n",
    "\n",
    "        rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd)#, pytest=pytest)\n",
    "\n",
    "    #ret = {'rgb_map' : rgb_map, 'disp_map' : disp_map, 'acc_map' : acc_map}\n",
    "    ret = {'rgb_map': rgb_map, 'disp_map': disp_map, 'acc_map': acc_map, 'depth_map': depth_map}\n",
    "\n",
    "    if retraw:\n",
    "        ret['raw'] = raw\n",
    "    if N_importance > 0:\n",
    "        ret['rgb0'] = rgb_map_0\n",
    "        ret['disp0'] = disp_map_0\n",
    "        ret['acc0'] = acc_map_0\n",
    "        ret['z_std'] = torch.std(z_samples, dim=-1, unbiased=False)  # [N_rays]\n",
    "    '''\n",
    "    for k in ret:\n",
    "        if (torch.isnan(ret[k]).any() or torch.isinf(ret[k]).any()) and DEBUG:\n",
    "            print(f\"! [Numerical Error] {k} contains nan or inf.\")\n",
    "    '''\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    K = None\n",
    "\n",
    "    if dataset_type == \"llff\":\n",
    "        images, poses, bds, render_poses, i_test = load_llff_data(basedir=data_dir, factor=factor, spherify=spherify)\n",
    "        focal = poses[0, 2, 4]\n",
    "        height, width = poses[0, :2, -1]\n",
    "        height, width = int(height), int(width)\n",
    "        hwf = [height, width, focal]\n",
    "        poses = poses[:, :3, :4]\n",
    "\n",
    "        print(f'Images shape: {images.shape}')\n",
    "        print(f'Poses shape: {poses.shape}')\n",
    "        print(f'Focal: {focal}')\n",
    "\n",
    "        if not isinstance(i_test, list):\n",
    "            i_test = [i_test]\n",
    "\n",
    "        if llff_hold > 0:\n",
    "            print(f'Set hold out frequency: {llff_hold}')\n",
    "            i_test = np.arange(images.shape[0])[::llff_hold]\n",
    "        i_val = i_test\n",
    "        n_training = np.array([i for i in np.arange(int(images.shape[0])) if (i not in i_test and i not in i_val)])\n",
    "\n",
    "        if use_ndc:\n",
    "            near, far = 0., 1.\n",
    "        else:\n",
    "            near = np.ndarray.min(bds) *.9\n",
    "            far = np.ndarray.max(bds) * 1.\n",
    "\n",
    "    print(f'Define near far: {near}, {far}')\n",
    "\n",
    "    print(f'Train views are: {n_training}')\n",
    "    print(f'Val views are: {i_val}')\n",
    "    print(f'Test views are: {i_test}')\n",
    "\n",
    "    '''\n",
    "    plt.imshow(images[i_val[0]])\n",
    "    print('Pose')\n",
    "    print(poses[i_val[0]])\n",
    "    '''\n",
    "    \n",
    "    if K is None:\n",
    "        K = np.array([\n",
    "            [focal, 0, 0.5*width],\n",
    "            [0, focal, 0.5*height],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "\n",
    "    if render_test:\n",
    "        render_poses = np.array(poses[i_test])\n",
    "\n",
    "    os.makedirs(os.path.join(basedir, expname), exist_ok=True)\n",
    "    # Create nerf model\n",
    "    render_kwargs_train, render_kwargs_test, start, grad_vars = init_models()\n",
    "    global_step = start\n",
    "\n",
    "    bds_dict = {\n",
    "        'near' : near,\n",
    "        'far' : far,\n",
    "    }\n",
    "    render_kwargs_train.update(bds_dict)\n",
    "    render_kwargs_test.update(bds_dict)\n",
    "\n",
    "    # Move testing data to GPU\n",
    "    render_poses = torch.Tensor(render_poses).to(device)\n",
    "\n",
    "    # Short circuit if only rendering out from trained model\n",
    "    #if args.render_only:\n",
    "    if True:\n",
    "        print('RENDER ONLY')\n",
    "        with torch.no_grad():\n",
    "            if render_test:\n",
    "                # render_test switches to test poses\n",
    "                images = images[i_test]\n",
    "            else:\n",
    "                # Default is smoother render_poses path\n",
    "                images = None\n",
    "\n",
    "            testsavedir = os.path.join(basedir, expname, 'renderonly_{}_{:06d}'.format('test' if render_test else 'path', start))\n",
    "            os.makedirs(testsavedir, exist_ok=True)\n",
    "            os.makedirs(os.path.join(testsavedir, 'rgb'), exist_ok=True)\n",
    "            os.makedirs(os.path.join(testsavedir, 'depth'), exist_ok=True)\n",
    "            print('test poses shape', render_poses.shape)\n",
    "\n",
    "            #rgbs, _ = render_path(render_poses, hwf, K, args.chunk, render_kwargs_test, gt_imgs=images, savedir=testsavedir, render_factor=args.render_factor)\n",
    "            rgbs, disps, depths = render_path(render_poses, hwf, K, chunksize, render_kwargs_test, gt_imgs=images, savedir=testsavedir, render_factor=render_factor)\n",
    "            print('Done rendering', testsavedir)\n",
    "\n",
    "            imageio.mimwrite(os.path.join(testsavedir, 'video.mp4'), to8b(rgbs), fps=30, quality=8)\n",
    "            \n",
    "            for idx, (rgb, depth) in enumerate(zip(rgbs, depths)):\n",
    "                imageio.imwrite(os.path.join(testsavedir, 'rgb', f\"{idx:06}.png\"), to8b(rgb))\n",
    "                cv2.imwrite(\n",
    "                    os.path.join(testsavedir, 'depth', f\"{idx:06}.png\"), \n",
    "                    (depth*1000).astype(np.uint16),\n",
    "                )\n",
    "\n",
    "\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    #torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerf_ntue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
