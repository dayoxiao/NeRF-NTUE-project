{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4286,"status":"ok","timestamp":1721795640958,"user":{"displayName":"蕭大祐","userId":"01697198556672762942"},"user_tz":-480},"id":"E68BI3zwYEiQ"},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","from torch import nn\n","import matplotlib.pyplot as plt\n","from tqdm import trange\n","\n","from model import *\n","from rays_util import *\n","from render import *\n","from load_llff import *"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1721795643330,"user":{"displayName":"蕭大祐","userId":"01697198556672762942"},"user_tz":-480},"id":"e--a6lvTHFn_"},"outputs":[],"source":["# For forward Passing\n","\n","def chunkify(\n","  inputs: torch.Tensor,\n","  chunksize: int = 2**15\n","):\n","  \"\"\"\n","  Divide input into chunks.\n","  Return: List(torch.Tensor)\n","  \"\"\"\n","  return [inputs[i:i+chunksize] for i in range(0, inputs.shape[0], chunksize)]\n","\n","def batchify_points(\n","  points: torch.Tensor,\n","  encoding_function: torch.Tensor,\n","  chunksize: int = 2**15\n","):\n","  \"\"\"\n","  Encode and chunkify \"points\" to prepare for NeRF model.\n","  Return: List[torch.Tensor]\n","  \"\"\"\n","  points = points.reshape((-1, 3))\n","  points = encoding_function(points)\n","  points = chunkify(points, chunksize=chunksize)\n","  return points\n","\n","def batchify_viewdirs(\n","  points: torch.Tensor,\n","  rays_d: torch.Tensor,\n","  encoding_function: torch.Tensor,\n","  chunksize: int = 2**15\n","):\n","  \"\"\"\n","  Encode and chunkify \"viewdirs\" to prepare for NeRF model.\n","  Return: List[torch.Tensor]\n","  \"\"\"\n","  viewdirs = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n","  viewdirs_flat = viewdirs[:, None, ...].expand(points.shape).reshape((-1, 3))\n","  embedded_viewdirs = encoding_function(viewdirs_flat)\n","  viewdirs = chunkify(embedded_viewdirs, chunksize=chunksize)\n","  return viewdirs"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1721795643330,"user":{"displayName":"蕭大祐","userId":"01697198556672762942"},"user_tz":-480},"id":"Q16c56DcPm4V"},"outputs":[],"source":["def nerf_forward(\n","  rays_o: torch.Tensor,\n","  rays_d: torch.Tensor,\n","  near: float,\n","  far: float,\n","  encoding_fn: torch.Tensor,\n","  coarse_model: nn.Module,\n","  fine_model = None,\n","  stratified_sampling_kwargs: dict = None,\n","  N_importance: int = 0,\n","  hierarchical_sampling_kwargs: dict = None,\n","  viewdirs_encoding_fn: torch.Tensor = None,\n","  chunksize: int = 2**15\n","):\n","  \"\"\"\n","  True full forward pass through all function and models\n","  Return: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]\n","  \"\"\"\n","\n","  # Set no kwargs if none are given.\n","  if stratified_sampling_kwargs is None:\n","    stratified_sampling_kwargs = {}\n","  if hierarchical_sampling_kwargs is None:\n","    hierarchical_sampling_kwargs = {}\n","\n","  # Strtified sampling(stage 1) for coarse query points.\n","  sample_pts, z_vals = sample_stratified(rays_o, rays_d, near, far, **stratified_sampling_kwargs)\n","\n","  # Prepare batches.\n","  batches = batchify_points(sample_pts, encoding_fn, chunksize=chunksize)\n","  if viewdirs_encoding_fn is not None:\n","    batches_viewdirs = batchify_viewdirs(sample_pts, rays_d,\n","                                               viewdirs_encoding_fn,\n","                                               chunksize=chunksize)\n","  else:\n","    batches_viewdirs = [None] * len(batches)\n","\n","  # Coarse model pass.\n","  # Split the encoded points into \"chunks\", run the model on all chunks\n","  # and concatenate the results (avoid OOM).\n","  predictions = []\n","  for batch, batch_viewdirs in zip(batches, batches_viewdirs):\n","    predictions.append(coarse_model(batch, viewdirs=batch_viewdirs))\n","  raw = torch.cat(predictions, dim=0)\n","  raw = raw.reshape(list(sample_pts.shape[:2]) + [raw.shape[-1]])\n","\n","  # Volume rendering\n","  rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(raw, z_vals, rays_d)\n","  outputs = {\n","      'z_vals_stratified': z_vals\n","  }\n","\n","  # Fine model pass.\n","  if N_importance > 0:\n","    # Save previous outputs to return.\n","    rgb_map_0, depth_map_0, acc_map_0 = rgb_map, depth_map, acc_map\n","\n","    # Apply hierarchical sampling(stage 2) for fine query points.\n","    sample_pts, z_vals_combined, z_hierarch = sample_hierarchical(\n","      rays_o, rays_d, z_vals, weights, N_importance,\n","      **hierarchical_sampling_kwargs)\n","\n","    # Prepare batches.\n","    batches = batchify_points(sample_pts, encoding_fn, chunksize=chunksize)\n","    if viewdirs_encoding_fn is not None:\n","      batches_viewdirs = batchify_viewdirs(sample_pts, rays_d,\n","                                                 viewdirs_encoding_fn,\n","                                                 chunksize=chunksize)\n","    else:\n","      batches_viewdirs = [None] * len(batches)\n","\n","    # Forward pass new samples through fine model.\n","    fine_model = fine_model if fine_model is not None else coarse_model\n","    predictions = []\n","    for batch, batch_viewdirs in zip(batches, batches_viewdirs):\n","      predictions.append(fine_model(batch, viewdirs=batch_viewdirs))\n","    raw = torch.cat(predictions, dim=0)\n","    raw = raw.reshape(list(sample_pts.shape[:2]) + [raw.shape[-1]])\n","\n","    # Volume rendering\n","    rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(raw, z_vals_combined, rays_d)\n","\n","    # Store outputs.\n","    outputs['z_vals_hierarchical'] = z_hierarch\n","    outputs['rgb_map_0'] = rgb_map_0\n","    outputs['depth_map_0'] = depth_map_0\n","    outputs['acc_map_0'] = acc_map_0\n","\n","  # Store outputs.\n","  outputs['rgb_map'] = rgb_map\n","  outputs['depth_map'] = depth_map\n","  outputs['acc_map'] = acc_map\n","  outputs['weights'] = weights\n","  #input()\n","  return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1721795643330,"user":{"displayName":"蕭大祐","userId":"01697198556672762942"},"user_tz":-480},"id":"UTOmAylpPqE7"},"outputs":[],"source":["\"\"\"\n","Hyperparameters\n","\"\"\"\n","\n","# Model\n","d_filter = 256          # Dimensions of linear layer filters\n","n_layers = 8            # Number of layers in network bottleneck\n","skip = [4]              # Layers at which to apply input residual\n","use_fine_model = True   # If set, creates a fine model\n","d_filter_fine = 256     # Dimensions of linear layer filters of fine network\n","n_layers_fine = 8       # Number of layers in fine network bottleneck\n","\n","# Stratified sampling\n","n_samples = 64          # Number of spatial samples per ray\n","perturb = True          # If set, applies noise to sample positions\n","inverse_depth = False   # If set, samples points linearly in inverse depth\n","\n","# Hierarchical sampling\n","N_importance = 64            # Number of samples per ray\n","perturb_hierarchical = True  # If set, applies noise to sample positions\n","\n","# Encoder\n","d_input = 3           # Number of input dimensions\n","n_freqs = 10          # Number of encoding functions for samples\n","log_sampling = True   # If set, frequencies scale in log space\n","use_viewdirs = True   # If set, use view direction as input\n","n_freqs_views = 4     # Number of encoding functions for views\n","\n","# Optimizer\n","lr = 5e-4             # Learning rate\n","lr_decay = 250        # Learning rate decay\n","decay_rate = 0.1      # decay rate of the learning rate decay\n","\n","# Training\n","n_iters = 10000+1           # Training iterations\n","batch_size = 1024           # Number of rays per gradient step (power of 2)\n","one_image_per_step = False  # One image per gradient step (disables batching)\n","chunksize = 1024            # Modify as needed to fit in GPU memory\n","center_crop = True          # Crop the center of image (one_image_per_)\n","center_crop_iters = 50      # Stop cropping center after this many epochs\n","display_rate = 100          # Display test output every X epochs\n","\n","# LLFF & Dataloading\n","start = 0                   # Starting iteration, 0 by default\n","basedir = \".\\\\logs\"         # Base directory for logs and ckpts\n","expname = \"fern_check\"       # Custom experiment name\n","datadir = \".\\\\fern\"         # Input data directory\n","llff_hold = 8               # if set, take image 1/N as test set\n","use_ndc = True              # use ndc for forward facing scenes\n","render_test = False         # render test set instead of custom poses\n","save_rate = 25           # frequency of saving the model by iteration\n","\n","# Early Stopping\n","warmup_iters = 100          # Number of iterations during warmup phase\n","warmup_min_fitness = 10.0   # Min val PSNR to continue training at warmup_iters\n","n_restarts = 10             # Number of times to restart if training stalls\n","\n","# Function kwargs\n","stratified_sampling_kwargs = {\n","    'n_samples': n_samples,\n","    'perturb': perturb,\n","    'inverse_depth': inverse_depth\n","}\n","hierarchical_sampling_kwargs = {\n","    'perturb': perturb_hierarchical\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1721795643330,"user":{"displayName":"蕭大祐","userId":"01697198556672762942"},"user_tz":-480},"id":"vWyOOscMP3H7"},"outputs":[],"source":["'''\n","Classes and Functions for training\n","'''\n","def crop_center(\n","  img: torch.Tensor,\n","  frac: float = 0.5\n","):\n","  \"\"\"\n","  Crop center square from image for better result.\n","  Return: torch.Tensor\n","  \"\"\"\n","  h_offset = round(img.shape[0] * (frac / 2))\n","  w_offset = round(img.shape[1] * (frac / 2))\n","  return img[h_offset:-h_offset, w_offset:-w_offset]\n","\n","class EarlyStopping:\n","  \"\"\"\n","  Early stopping helper.\n","  \"\"\"\n","  def __init__(\n","    self,\n","    patience: int = 50,\n","    min_improve: float = 1e-4\n","  ):\n","    self.best_fitness = 0.0  # PSNR\n","    self.best_iter = 0\n","    self.min_improve = min_improve\n","    self.patience = patience or float('inf')  # number of epochs to wait if fitness stop improving\n","\n","  def __call__(\n","    self,\n","    iter: int,\n","    fitness: float\n","  ):\n","    \"\"\"\n","    Check if criterion for stopping is met.\n","    Return: Bool\n","    \"\"\"\n","    if (fitness - self.best_fitness) > self.min_improve:\n","      self.best_iter = iter\n","      self.best_fitness = fitness\n","    delta = iter - self.best_iter\n","    stop = delta >= self.patience  # stop training\n","    return stop"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_dir = \"H:\\\\NeRF-NTUE-project\\\\fern\"\n","images, poses, bds, render_poses, i_test = load_llff_data(data_dir)\n","focal = poses[0, 2, 4]\n","height, width = poses[0, :2, -1]\n","height, width = int(height), int(width)\n","poses = poses[:, :3, :4]\n","\n","print(f'Images shape: {images.shape}')\n","print(f'Poses shape: {poses.shape}')\n","print(f'Focal: {focal}')\n","\n","if not isinstance(i_test, list):\n","    i_test = [i_test]\n","\n","if llff_hold > 0:\n","    print(f'Set hold out frequency: {llff_hold}')\n","    i_test = np.arange(images.shape[0])[::llff_hold]\n","i_val = i_test\n","n_training = np.array([i for i in np.arange(int(images.shape[0])) if (i not in i_test and i not in i_val)])\n","\n","if use_ndc:\n","    near, far = 0., 1.\n","else:\n","    near = np.ndarray.min(bds) *.9\n","    far = np.ndarray.max(bds) * 1.\n","\n","print(f'Define near far: {near}, {far}')\n","\n","print(f'Train views are: {n_training}')\n","print(f'Val views are: {i_val}')\n","print(f'Test views are: {i_test}')\n","\n","plt.imshow(images[i_val[0]])\n","print('Pose')\n","print(poses[i_val[0]])\n","\n","if render_test:\n","    render_poses = np.array(poses[i_test])\n","\n","os.makedirs(os.path.join(basedir, expname), exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1721795643330,"user":{"displayName":"蕭大祐","userId":"01697198556672762942"},"user_tz":-480},"id":"yS4OBurHQEjC"},"outputs":[],"source":["def init_models():\n","  \"\"\"\n","  Initialize models, encoders, and optimizer for training.\n","  \"\"\"\n","\n","  # Encoders\n","  encoder = PositionalEncoder(d_input, n_freqs, log_sampling=log_sampling)\n","  encode = lambda x: encoder(x)\n","\n","  # View direction encoders\n","  if use_viewdirs:\n","    encoder_viewdirs = PositionalEncoder(d_input, n_freqs_views,\n","                                        log_sampling=log_sampling)\n","    encode_viewdirs = lambda x: encoder_viewdirs(x)\n","    d_viewdirs = encoder_viewdirs.d_output\n","    test_dir = d_viewdirs\n","  else:\n","    encode_viewdirs = None\n","    d_viewdirs = None\n","\n","  # Models\n","  model = NeRF(encoder.d_output, n_layers=n_layers, d_filter=d_filter, skip=skip,\n","              d_viewdirs=d_viewdirs)\n","  model.to(device)\n","  model_params = list(model.parameters())\n","  if use_fine_model:\n","    fine_model = NeRF(encoder.d_output, n_layers=n_layers_fine, d_filter=d_filter, skip=skip,\n","                      d_viewdirs=d_viewdirs)\n","    fine_model.to(device)\n","    model_params = model_params + list(fine_model.parameters())\n","  else:\n","    fine_model = None\n","\n","  # Optimizer\n","  optimizer = torch.optim.Adam(model_params, lr=lr)\n","\n","  # Early Stopping\n","  warmup_stopper = EarlyStopping(patience=50)\n","\n","  # Checkpoints loading\n","  os.makedirs(os.path.join(basedir, expname), exist_ok=True)\n","  ckpts = [os.path.join(basedir, expname, f) for f in sorted(os.listdir(os.path.join(basedir, expname))) if 'tar' in f]\n","  print(\"Found ckpts\", ckpts)\n","\n","  if len(ckpts) > 0:\n","    ckpt_path = ckpts[-1]\n","    print('Reloading from', ckpt_path)\n","    ckpt = torch.load(ckpt_path)\n","\n","    global start\n","    start = ckpt['global_step']\n","    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n","\n","    # Load model\n","    model.load_state_dict(ckpt['coarse_model_dict'])\n","    if fine_model is not None:\n","        fine_model.load_state_dict(ckpt['fine_model_dict'])\n","  \n","\n","  return model, fine_model, encode, encode_viewdirs, optimizer, warmup_stopper"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# To torch tensors\n","images = torch.from_numpy(images[n_training]).to(device)\n","poses = torch.from_numpy(poses).to(device)\n","#focal = torch.from_numpy(focal).to(device)\n","#testimg = torch.from_numpy(data['images'][testimg_idx]).to(device)\n","#testpose = torch.from_numpy(data['poses'][testimg_idx]).to(device)\n","\n","testimg = images[i_test].to(device)\n","testpose = poses[i_test].to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1721795643330,"user":{"displayName":"蕭大祐","userId":"01697198556672762942"},"user_tz":-480},"id":"QeZ6vf13QHQf"},"outputs":[],"source":["#Training Loop\n","\n","def train():\n","  \"\"\"\n","  Launch training.\n","  \"\"\"\n","  global start\n","  global_step = start\n","\n","  # Shuffle rays across all images.\n","\n","  all_ndc_rays = []\n","\n","  if not one_image_per_step:\n","    height, width = images.shape[1:3]\n","\n","    \n","    for p in poses[n_training]:\n","      rays_o, rays_d = get_rays(height, width, focal, p)\n","      rays_o, rays_d = ndc_rays(height, width, focal, 1., rays_o, rays_d)\n","      rays_od = torch.stack((rays_o, rays_d), dim=0)\n","      all_ndc_rays.append(rays_od)\n","    all_ndc_rays = torch.stack(all_ndc_rays, dim=0)\n","\n","    #all_rays = torch.stack([torch.stack(get_rays(height, width, focal, p), dim=0)\n","                        #for p in poses[:n_training]], dim=0)  # [100, 2, 100, 100, 3]\n","    \n","    rays_rgb = torch.cat([all_ndc_rays, images[:, None]], 1)  # [100, 3, 100, 100, 3]\n","\n","    rays_rgb = torch.permute(rays_rgb, [0, 2, 3, 1, 4]) # [100, 100, 100, 3, 3]\n","\n","    rays_rgb = rays_rgb.reshape([-1, 3, 3]) # [1000000, 3, 3]\n","\n","    rays_rgb = rays_rgb.type(torch.float32)\n","    rays_rgb = rays_rgb[torch.randperm(rays_rgb.shape[0])]\n","    i_batch = 0\n","\n","  train_psnrs = []\n","  start = start + 1\n","  for i in trange(start, n_iters):\n","    model.train()\n","\n","    if one_image_per_step:\n","      # Randomly pick an image as target.\n","      target_img_idx = np.random.randint(images.shape[0])\n","      target_img = images[target_img_idx].to(device)\n","      if center_crop and i < center_crop_iters:\n","        target_img = crop_center(target_img)\n","      height, width = target_img.shape[:2]\n","      target_pose = poses[target_img_idx].to(device)\n","      rays_o, rays_d = get_rays(height, width, focal, target_pose)\n","      ###\n","      rays_o, rays_d = ndc_rays(height, width, focal, 1., rays_o, rays_d)\n","      ###\n","      rays_o = rays_o.reshape([-1, 3])\n","      rays_d = rays_d.reshape([-1, 3])\n","    else:\n","      # Random over all images.\n","      batch = rays_rgb[i_batch:i_batch + batch_size]\n","      batch = torch.transpose(batch, 0, 1)  # [3, 2**14, 3]\n","      rays_o, rays_d, target_img = batch  # [2**14, 3]\n","      height, width = target_img.shape[:2]\n","      i_batch += batch_size\n","      # Shuffle after one epoch\n","      if i_batch >= rays_rgb.shape[0]:\n","          print(\"An epoch ends, shuffle all data!\")\n","          rays_rgb = rays_rgb[torch.randperm(rays_rgb.shape[0])]\n","          i_batch = 0\n","\n","    target_img = target_img.reshape([-1, 3])\n","\n","    # Run one iteration and get the rendered RGB image.\n","    outputs = nerf_forward(rays_o, rays_d,\n","                           near, far, encode, model,\n","                           fine_model=fine_model,\n","                           stratified_sampling_kwargs=stratified_sampling_kwargs,\n","                           N_importance=N_importance,\n","                           hierarchical_sampling_kwargs=hierarchical_sampling_kwargs,\n","                           viewdirs_encoding_fn=encode_viewdirs,\n","                           chunksize=chunksize)\n","\n","    # Check for any numerical issues.\n","    for k, v in outputs.items():\n","      if torch.isnan(v).any():\n","        print(f\"! [Numerical Alert] {k} contains NaN.\")\n","      if torch.isinf(v).any():\n","        print(f\"! [Numerical Alert] {k} contains Inf.\")\n","\n","    # Backprop\n","    rgb_predicted = outputs['rgb_map']\n","    optimizer.zero_grad()\n","    loss = torch.nn.functional.mse_loss(rgb_predicted, target_img)\n","    psnr = -10. * torch.log10(loss)\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","    train_psnrs.append(psnr.item())\n","\n","    # Update learning rate\n","    decay_steps = lr_decay * 1000\n","    new_lrate = lr * (decay_rate ** (global_step / decay_steps))\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = new_lrate\n","\n","    # Logging.\n","    if i % display_rate == 0:\n","      print(f\"Step: {global_step}, Loss: {loss.item()}, psnr: {psnr.item()}\")\n","      \n","    if i % save_rate == 0:\n","      path = os.path.join(basedir, expname, '{0:06d}.tar'.format(i))\n","      torch.save({\n","          'global_step': global_step,\n","          'coarse_model_dict': model.state_dict(),\n","          'fine_model_dict': fine_model.state_dict(),\n","          'optimizer_state_dict': optimizer.state_dict(),\n","      }, path)\n","      print('Saved checkpoints at', path)\n","\n","\n","    # Check PSNR for issues and stop if any are found.\n","    '''\n","    if i == warmup_iters - 1:\n","      if val_psnr < warmup_min_fitness:\n","        print(f'Val PSNR {val_psnr} below warmup_min_fitness {warmup_min_fitness}. Stopping...')\n","        return False, train_psnrs, val_psnrs\n","    '''\n","    if i < warmup_iters:\n","      if warmup_stopper is not None and warmup_stopper(i, psnr):\n","        print(f'Train PSNR flatlined at {psnr} for {warmup_stopper.patience} iters. Stopping...')\n","        return False, train_psnrs \n","      \n","    \n","    global_step += 1\n","\n","  return True, train_psnrs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oAwZZkrgQM6J","outputId":"15735728-90ac-445c-dc9a-919998a98ffe"},"outputs":[],"source":["# Training session\n","for _ in range(n_restarts):\n","  model, fine_model, encode, encode_viewdirs, optimizer, warmup_stopper = init_models()\n","  success, train_psnrs = train()\n","  #if success and val_psnrs[-1] >= warmup_min_fitness:\n","  if success:\n","    print('Training successful!')\n","    break\n","\n","print('')\n","print(f'Done!')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vz0vmQEX5Ffh"},"outputs":[],"source":["! nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"139O5ZF0ehmNo25mujWELZ8hbBktYXPrY","timestamp":1721308408797}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
